# -*- coding: utf-8 -*-
"""Email spam classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZFJ1BaUIH5NmG9IZOU5anLF9Hy_3MVYv
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import nltk
df=pd.read_csv('spam.csv',encoding='latin-1')
df.sample(5)
df.shape

df.info()

"""//DATACLEANING"""

df.sample(5)

df.rename(columns={'v1':'sample','v2':'text'},inplace=True)
df.sample(5)
encoder=LabelEncoder()

df['sample']=encoder.fit_transform(df['sample'])
df.sample(5)

df.head()

df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)

df.head()

df.isnull().sum()
df.duplicated().sum()

df=df.drop_duplicates(keep='first')
df.shape

df.head()
df['sample'].value_counts()

import matplotlib.pyplot as plt
plt.pie(df['sample'].value_counts(), labels=['ham','spam'],autopct="%0.3f")
plt.show()

"""DATA PREPROCESSING : TEXTUAL DATA"""

df['characters']=df['text'].apply(len)
df.head()

nltk.download('punkt')

df['words']=df['text'].apply(lambda x:len(nltk.word_tokenize(x)))
df.head()

df['sentenc']=df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))
df.head()

df[['characters','words','sentenc']].describe()

df[df['sample']==0][['characters','words','sentenc']].describe()

df[df['sample']==1][['characters','words','sentenc']].describe()

import seaborn as sns
plt.figure(figsize=(5,4))
sns.histplot(df[df['sample']==0]['characters'],color='yellow')
sns.histplot(df[df['sample']==1]['characters'],color='green')

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import string

nltk.download('stopwords')

ps = PorterStemmer()

def preprocess_text(input_text):
    input_text = input_text.lower()
    words = nltk.word_tokenize(input_text)
    cleaned_words = [word for word in words if word.isalnum()]
    processed_words = [ps.stem(word) for word in cleaned_words if word not in stopwords.words('english')]
    return " ".join(processed_words)

df['processed_text'] = df['text'].apply(preprocess_text)
df.head()

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf=TfidfVectorizer()
a=tfidf.fit_transform(df['processed_text']).toarray()
a.shape

b=df['sample'].values
b.shape

"""CHOOSING MULTINOMIAL,PRECISION IS BEST"""

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score
x_train,x_test,y_train,y_test=train_test_split(a,b,test_size=0.2,random_state=2)
mnb=MultinomialNB()
mnb.fit(x_train,y_train)
y_pred=mnb.predict(x_test)
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(precision_score(y_test,y_pred))

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
log_reg = LogisticRegression()
log_reg.fit(x_train, y_train)
predictions_lr = log_reg.predict(x_test)
print(accuracy_score(y_test, predictions_lr))
print(confusion_matrix(y_test, predictions_lr))
print(precision_score(y_test, predictions_lr))

svc_model = SVC()
svc_model.fit(x_train, y_train)
predictions_svc = svc_model.predict(x_test)
print(accuracy_score(y_test, predictions_svc))
print(confusion_matrix(y_test, predictions_svc))
print(precision_score(y_test, predictions_svc))

lr_metrics = [accuracy_score(y_test, predictions_lr), precision_score(y_test, predictions_lr)]
svc_metrics = [accuracy_score(y_test, predictions_svc), precision_score(y_test, predictions_svc)]
labels = ['Accuracy', 'Precision']
x = range(len(labels))
plt.bar(x, lr_metrics, width=0.2, label='Logistic Regression', align='center')
plt.bar(x, svc_metrics, width=0.2, label='SVC', align='edge')
plt.xticks(x, labels)
plt.ylabel('Score')
plt.title('Model Comparison: Logistic Regression vs SVC')
plt.legend()
plt.show()